{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple two-layer bidirectional LSTM with Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import copy\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import KFold\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 250\n",
    "lr = 0.01\n",
    "n_folds = 5\n",
    "lstm_input_size = 1\n",
    "hidden_state_size = 30\n",
    "batch_size = 30\n",
    "num_sequence_layers = 2\n",
    "output_dim = 11\n",
    "num_time_steps = 4000\n",
    "rnn_type = 'LSTM'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bi_RNN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, batch_size, output_dim=11, num_layers=2, rnn_type='LSTM'):\n",
    "        super(Bi_RNN, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        #Define the initial linear hidden layer\n",
    "        self.init_linear = nn.Linear(self.input_dim, self.input_dim)\n",
    "\n",
    "        # Define the LSTM layer\n",
    "        self.lstm = eval('nn.' + rnn_type)(self.input_dim, self.hidden_dim, self.num_layers, batch_first=True, bidirectional=True)\n",
    "\n",
    "        # Define the output layer\n",
    "        self.linear = nn.Linear(self.hidden_dim * 2, output_dim)\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # This is what we'll initialise our hidden state as\n",
    "        return (torch.zeros(self.num_layers, self.batch_size, self.hidden_dim),\n",
    "                torch.zeros(self.num_layers, self.batch_size, self.hidden_dim))\n",
    "\n",
    "    def forward(self, input):\n",
    "        #Forward pass through initial hidden layer\n",
    "        linear_input = self.init_linear(input)\n",
    "\n",
    "        # Forward pass through LSTM layer\n",
    "        # shape of lstm_out: [batch_size, input_size ,hidden_dim]\n",
    "        # shape of self.hidden: (a, b), where a and b both\n",
    "        # have shape (batch_size, num_layers, hidden_dim).\n",
    "        lstm_out, self.hidden = self.lstm(linear_input)\n",
    "\n",
    "        # Can pass on the entirety of lstm_out to the next layer if it is a seq2seq prediction\n",
    "        y_pred = self.linear(lstm_out)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ION_Dataset_Sequential(Dataset):\n",
    "    def __init__(self, input, output):\n",
    "        self.input = input\n",
    "        self.output = output\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.input[idx]\n",
    "        y = self.output[idx]\n",
    "        x = torch.tensor(x, dtype=torch.float)\n",
    "        y = torch.tensor(y, dtype=torch.float)\n",
    "        return x, y\n",
    "\n",
    "class ION_Dataset_Sequential_test(Dataset):\n",
    "    def __init__(self, input):\n",
    "        self.input = input\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.input[idx]\n",
    "        x = torch.tensor(x, dtype=torch.float)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data\n",
    "\n",
    "We removed the drift following https://www.kaggle.com/cdeotte/one-feature-model-0-930/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('/kaggle/input/data-no-drift/train_detrend.csv')\n",
    "test_df = pd.read_csv('/kaggle/input/data-no-drift/test_detrend.csv')\n",
    "X = train_df['signal'].values.reshape(-1, num_time_steps, 1)\n",
    "y = pd.get_dummies(train_df['open_channels']).values.reshape(-1, num_time_steps, output_dim)\n",
    "test_input = test_df[\"signal\"].values.reshape(-1, num_time_steps, 1)\n",
    "train_input_mean = X.mean()\n",
    "train_input_sigma = X.std()\n",
    "test_input = (test_input-train_input_mean)/train_input_sigma\n",
    "test_preds = np.zeros((int(test_input.shape[0] * test_input.shape[1])))\n",
    "test = ION_Dataset_Sequential_test(test_input)\n",
    "test_loader = DataLoader(test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, evaluate with 5-fold CV and keep best model on every fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting fold 0\n",
      "###### splitting and reshaping the data\n",
      "(1000, 4000, 1)\n",
      "###### Loading\n",
      "Epoch 1/250 \t loss=0.3245 \t train_f1=0.0533 \t val_loss=0.2822 \t val_f1=0.0366 \t time=20.61s\n",
      "Epoch 11/250 \t loss=0.0325 \t train_f1=0.8263 \t val_loss=0.0360 \t val_f1=0.8259 \t time=19.30s\n",
      "Epoch 21/250 \t loss=0.0222 \t train_f1=0.9200 \t val_loss=0.0272 \t val_f1=0.9126 \t time=19.29s\n",
      "Epoch 31/250 \t loss=0.0210 \t train_f1=0.9221 \t val_loss=0.0275 \t val_f1=0.9094 \t time=19.24s\n",
      "Epoch 41/250 \t loss=0.0200 \t train_f1=0.9250 \t val_loss=0.0229 \t val_f1=0.9233 \t time=20.48s\n",
      "Epoch 51/250 \t loss=0.0197 \t train_f1=0.9241 \t val_loss=0.0227 \t val_f1=0.9240 \t time=19.00s\n",
      "Epoch 61/250 \t loss=0.0197 \t train_f1=0.9235 \t val_loss=0.0229 \t val_f1=0.9220 \t time=19.14s\n",
      "Epoch 71/250 \t loss=0.0197 \t train_f1=0.9243 \t val_loss=0.0226 \t val_f1=0.9229 \t time=19.07s\n",
      "Epoch 81/250 \t loss=0.0195 \t train_f1=0.9242 \t val_loss=0.0239 \t val_f1=0.9144 \t time=20.03s\n",
      "Epoch 91/250 \t loss=0.0225 \t train_f1=0.9192 \t val_loss=0.0249 \t val_f1=0.9194 \t time=19.57s\n",
      "Epoch 101/250 \t loss=0.0190 \t train_f1=0.9230 \t val_loss=0.0213 \t val_f1=0.9265 \t time=19.52s\n",
      "Epoch 111/250 \t loss=0.0179 \t train_f1=0.9264 \t val_loss=0.0212 \t val_f1=0.9261 \t time=19.01s\n",
      "Epoch 121/250 \t loss=0.0191 \t train_f1=0.9250 \t val_loss=0.0233 \t val_f1=0.9221 \t time=19.89s\n",
      "Epoch 131/250 \t loss=0.0186 \t train_f1=0.9256 \t val_loss=0.0220 \t val_f1=0.9244 \t time=19.58s\n",
      "Epoch 141/250 \t loss=0.0179 \t train_f1=0.9273 \t val_loss=0.0218 \t val_f1=0.9202 \t time=19.50s\n",
      "Epoch 151/250 \t loss=0.0404 \t train_f1=0.8610 \t val_loss=0.0362 \t val_f1=0.9026 \t time=19.21s\n",
      "Epoch 161/250 \t loss=0.0206 \t train_f1=0.9212 \t val_loss=0.0238 \t val_f1=0.9158 \t time=19.73s\n",
      "Epoch 171/250 \t loss=0.0188 \t train_f1=0.9252 \t val_loss=0.0223 \t val_f1=0.9218 \t time=19.45s\n",
      "Epoch 181/250 \t loss=0.0190 \t train_f1=0.9251 \t val_loss=0.0224 \t val_f1=0.9184 \t time=19.20s\n",
      "Epoch 191/250 \t loss=0.0187 \t train_f1=0.9229 \t val_loss=0.0213 \t val_f1=0.9249 \t time=19.29s\n",
      "Epoch 201/250 \t loss=0.0183 \t train_f1=0.9263 \t val_loss=0.0216 \t val_f1=0.9256 \t time=20.06s\n",
      "Epoch 211/250 \t loss=0.0181 \t train_f1=0.9274 \t val_loss=0.0217 \t val_f1=0.9247 \t time=20.00s\n",
      "Epoch 221/250 \t loss=0.0186 \t train_f1=0.9258 \t val_loss=0.0213 \t val_f1=0.9266 \t time=19.47s\n",
      "Epoch 231/250 \t loss=0.0186 \t train_f1=0.9255 \t val_loss=0.0217 \t val_f1=0.9252 \t time=19.37s\n",
      "Epoch 241/250 \t loss=0.0174 \t train_f1=0.9286 \t val_loss=0.0217 \t val_f1=0.9260 \t time=20.11s\n",
      "BEST VALIDATION SCORE (F1):  0.9275561937147053\n",
      "starting fold 1\n",
      "###### splitting and reshaping the data\n",
      "(1000, 4000, 1)\n",
      "###### Loading\n",
      "Epoch 1/250 \t loss=0.3191 \t train_f1=0.0606 \t val_loss=0.2860 \t val_f1=0.0323 \t time=19.19s\n",
      "Epoch 11/250 \t loss=0.1350 \t train_f1=0.6671 \t val_loss=0.1140 \t val_f1=0.6634 \t time=19.37s\n",
      "Epoch 21/250 \t loss=0.0330 \t train_f1=0.9024 \t val_loss=0.0391 \t val_f1=0.9036 \t time=20.25s\n",
      "Epoch 31/250 \t loss=0.0249 \t train_f1=0.9153 \t val_loss=0.0315 \t val_f1=0.9118 \t time=20.33s\n",
      "Epoch 41/250 \t loss=0.0228 \t train_f1=0.9181 \t val_loss=0.0291 \t val_f1=0.9176 \t time=19.26s\n",
      "Epoch 51/250 \t loss=0.0228 \t train_f1=0.9194 \t val_loss=0.0284 \t val_f1=0.9185 \t time=19.00s\n",
      "Epoch 61/250 \t loss=0.0211 \t train_f1=0.9219 \t val_loss=0.0287 \t val_f1=0.9168 \t time=19.85s\n",
      "Epoch 71/250 \t loss=0.0207 \t train_f1=0.9212 \t val_loss=0.0267 \t val_f1=0.9211 \t time=19.43s\n",
      "Epoch 81/250 \t loss=0.0207 \t train_f1=0.9228 \t val_loss=0.0258 \t val_f1=0.9185 \t time=19.20s\n",
      "Epoch 91/250 \t loss=0.0206 \t train_f1=0.9224 \t val_loss=0.0319 \t val_f1=0.9126 \t time=18.90s\n",
      "Epoch 101/250 \t loss=0.0201 \t train_f1=0.9234 \t val_loss=0.0259 \t val_f1=0.9215 \t time=18.93s\n",
      "Epoch 111/250 \t loss=0.0216 \t train_f1=0.9186 \t val_loss=0.0273 \t val_f1=0.9190 \t time=19.79s\n",
      "Epoch 121/250 \t loss=0.0284 \t train_f1=0.9057 \t val_loss=0.0325 \t val_f1=0.9115 \t time=19.19s\n",
      "Epoch 131/250 \t loss=0.0210 \t train_f1=0.9214 \t val_loss=0.0281 \t val_f1=0.9189 \t time=18.99s\n",
      "Epoch 141/250 \t loss=0.0211 \t train_f1=0.9190 \t val_loss=0.0269 \t val_f1=0.9184 \t time=19.32s\n",
      "Epoch 151/250 \t loss=0.0204 \t train_f1=0.9224 \t val_loss=0.0262 \t val_f1=0.9209 \t time=19.70s\n",
      "Epoch 161/250 \t loss=0.0203 \t train_f1=0.9224 \t val_loss=0.0257 \t val_f1=0.9219 \t time=19.67s\n",
      "Epoch 171/250 \t loss=0.0203 \t train_f1=0.9221 \t val_loss=0.0268 \t val_f1=0.9179 \t time=18.92s\n",
      "Epoch 181/250 \t loss=0.0199 \t train_f1=0.9236 \t val_loss=0.0253 \t val_f1=0.9178 \t time=19.04s\n",
      "Epoch 191/250 \t loss=0.0203 \t train_f1=0.9233 \t val_loss=0.0253 \t val_f1=0.9216 \t time=19.10s\n",
      "Epoch 201/250 \t loss=0.0200 \t train_f1=0.9236 \t val_loss=0.0255 \t val_f1=0.9223 \t time=19.52s\n",
      "Epoch 211/250 \t loss=0.0200 \t train_f1=0.9236 \t val_loss=0.0257 \t val_f1=0.9227 \t time=19.42s\n",
      "Epoch 221/250 \t loss=0.0206 \t train_f1=0.9214 \t val_loss=0.0263 \t val_f1=0.9168 \t time=18.98s\n",
      "Epoch 231/250 \t loss=0.0204 \t train_f1=0.9229 \t val_loss=0.0248 \t val_f1=0.9213 \t time=19.01s\n",
      "Epoch 241/250 \t loss=0.0194 \t train_f1=0.9242 \t val_loss=0.0251 \t val_f1=0.9191 \t time=18.84s\n",
      "BEST VALIDATION SCORE (F1):  0.9235153624787974\n",
      "starting fold 2\n",
      "###### splitting and reshaping the data\n",
      "(1000, 4000, 1)\n",
      "###### Loading\n",
      "Epoch 1/250 \t loss=0.3244 \t train_f1=0.0576 \t val_loss=0.2819 \t val_f1=0.0301 \t time=19.42s\n",
      "Epoch 11/250 \t loss=0.0370 \t train_f1=0.8230 \t val_loss=0.0379 \t val_f1=0.8238 \t time=19.43s\n",
      "Epoch 21/250 \t loss=0.0231 \t train_f1=0.8814 \t val_loss=0.0249 \t val_f1=0.9133 \t time=19.04s\n",
      "Epoch 31/250 \t loss=0.0213 \t train_f1=0.9180 \t val_loss=0.0227 \t val_f1=0.9248 \t time=19.09s\n",
      "Epoch 41/250 \t loss=0.0205 \t train_f1=0.9218 \t val_loss=0.0226 \t val_f1=0.9265 \t time=19.00s\n",
      "Epoch 51/250 \t loss=0.0210 \t train_f1=0.9182 \t val_loss=0.0213 \t val_f1=0.9274 \t time=19.52s\n",
      "Epoch 61/250 \t loss=0.0202 \t train_f1=0.9186 \t val_loss=0.0214 \t val_f1=0.9262 \t time=19.79s\n",
      "Epoch 71/250 \t loss=0.0201 \t train_f1=0.9229 \t val_loss=0.0221 \t val_f1=0.9224 \t time=19.00s\n",
      "Epoch 81/250 \t loss=0.0198 \t train_f1=0.9232 \t val_loss=0.0230 \t val_f1=0.9217 \t time=18.83s\n",
      "Epoch 91/250 \t loss=0.0193 \t train_f1=0.9241 \t val_loss=0.0224 \t val_f1=0.9229 \t time=19.10s\n",
      "Epoch 101/250 \t loss=0.0202 \t train_f1=0.9203 \t val_loss=0.0248 \t val_f1=0.9216 \t time=20.10s\n",
      "Epoch 111/250 \t loss=0.0190 \t train_f1=0.9245 \t val_loss=0.0210 \t val_f1=0.9281 \t time=19.09s\n",
      "Epoch 121/250 \t loss=0.0191 \t train_f1=0.9238 \t val_loss=0.0216 \t val_f1=0.9267 \t time=18.82s\n",
      "Epoch 131/250 \t loss=0.0185 \t train_f1=0.9260 \t val_loss=0.0210 \t val_f1=0.9280 \t time=18.96s\n",
      "Epoch 141/250 \t loss=0.0185 \t train_f1=0.9257 \t val_loss=0.0206 \t val_f1=0.9273 \t time=18.94s\n",
      "Epoch 151/250 \t loss=0.0191 \t train_f1=0.9239 \t val_loss=0.0219 \t val_f1=0.9235 \t time=19.60s\n",
      "Epoch 161/250 \t loss=0.0189 \t train_f1=0.9251 \t val_loss=0.0204 \t val_f1=0.9285 \t time=19.65s\n",
      "Epoch 171/250 \t loss=0.0180 \t train_f1=0.9266 \t val_loss=0.0225 \t val_f1=0.9211 \t time=18.95s\n",
      "Epoch 181/250 \t loss=0.0186 \t train_f1=0.9258 \t val_loss=0.0212 \t val_f1=0.9272 \t time=18.89s\n",
      "Epoch 191/250 \t loss=0.0189 \t train_f1=0.9257 \t val_loss=0.0204 \t val_f1=0.9293 \t time=19.64s\n",
      "Epoch 201/250 \t loss=0.0185 \t train_f1=0.9242 \t val_loss=0.0213 \t val_f1=0.9283 \t time=19.83s\n",
      "Epoch 211/250 \t loss=0.0202 \t train_f1=0.9225 \t val_loss=0.0212 \t val_f1=0.9271 \t time=18.95s\n",
      "Epoch 221/250 \t loss=0.0180 \t train_f1=0.9243 \t val_loss=0.0204 \t val_f1=0.9255 \t time=18.91s\n",
      "Epoch 231/250 \t loss=0.0179 \t train_f1=0.9263 \t val_loss=0.0209 \t val_f1=0.9270 \t time=19.15s\n",
      "Epoch 241/250 \t loss=0.0176 \t train_f1=0.9276 \t val_loss=0.0197 \t val_f1=0.9294 \t time=19.42s\n",
      "BEST VALIDATION SCORE (F1):  0.9303433252095854\n",
      "starting fold 3\n",
      "###### splitting and reshaping the data\n",
      "(1000, 4000, 1)\n",
      "###### Loading\n",
      "Epoch 1/250 \t loss=0.3282 \t train_f1=0.0542 \t val_loss=0.2766 \t val_f1=0.0393 \t time=19.25s\n",
      "Epoch 11/250 \t loss=0.0519 \t train_f1=0.7038 \t val_loss=0.0518 \t val_f1=0.7109 \t time=19.14s\n",
      "Epoch 21/250 \t loss=0.0270 \t train_f1=0.8475 \t val_loss=0.0274 \t val_f1=0.9026 \t time=19.67s\n",
      "Epoch 31/250 \t loss=0.0239 \t train_f1=0.9150 \t val_loss=0.0234 \t val_f1=0.9197 \t time=19.52s\n",
      "Epoch 41/250 \t loss=0.0217 \t train_f1=0.9204 \t val_loss=0.0220 \t val_f1=0.9249 \t time=19.90s\n",
      "Epoch 51/250 \t loss=0.0212 \t train_f1=0.9208 \t val_loss=0.0220 \t val_f1=0.9251 \t time=19.15s\n",
      "Epoch 61/250 \t loss=0.0215 \t train_f1=0.9203 \t val_loss=0.0221 \t val_f1=0.9253 \t time=19.15s\n",
      "Epoch 71/250 \t loss=0.0206 \t train_f1=0.9215 \t val_loss=0.0220 \t val_f1=0.9241 \t time=18.88s\n",
      "Epoch 81/250 \t loss=0.0206 \t train_f1=0.9207 \t val_loss=0.0220 \t val_f1=0.9245 \t time=19.53s\n",
      "Epoch 91/250 \t loss=0.0205 \t train_f1=0.9229 \t val_loss=0.0217 \t val_f1=0.9175 \t time=19.89s\n",
      "Epoch 101/250 \t loss=0.0201 \t train_f1=0.9218 \t val_loss=0.0209 \t val_f1=0.9249 \t time=19.06s\n",
      "Epoch 111/250 \t loss=0.0209 \t train_f1=0.9216 \t val_loss=0.0225 \t val_f1=0.9029 \t time=19.00s\n",
      "Epoch 121/250 \t loss=0.0202 \t train_f1=0.9204 \t val_loss=0.0202 \t val_f1=0.9262 \t time=19.30s\n",
      "Epoch 131/250 \t loss=0.0192 \t train_f1=0.9252 \t val_loss=0.0197 \t val_f1=0.9278 \t time=19.84s\n",
      "Epoch 141/250 \t loss=0.0196 \t train_f1=0.9240 \t val_loss=0.0203 \t val_f1=0.9251 \t time=19.19s\n",
      "Epoch 151/250 \t loss=0.0196 \t train_f1=0.9230 \t val_loss=0.0205 \t val_f1=0.9227 \t time=19.03s\n",
      "Epoch 161/250 \t loss=0.0186 \t train_f1=0.9261 \t val_loss=0.0193 \t val_f1=0.9296 \t time=18.91s\n",
      "Epoch 171/250 \t loss=0.0182 \t train_f1=0.9262 \t val_loss=0.0190 \t val_f1=0.9295 \t time=18.84s\n",
      "Epoch 181/250 \t loss=0.0187 \t train_f1=0.9254 \t val_loss=0.0197 \t val_f1=0.9234 \t time=19.70s\n",
      "Epoch 191/250 \t loss=0.0182 \t train_f1=0.9269 \t val_loss=0.0192 \t val_f1=0.9272 \t time=19.37s\n",
      "Epoch 201/250 \t loss=0.0182 \t train_f1=0.9248 \t val_loss=0.0190 \t val_f1=0.9294 \t time=19.04s\n",
      "Epoch 211/250 \t loss=0.0206 \t train_f1=0.9204 \t val_loss=0.0203 \t val_f1=0.9279 \t time=19.07s\n",
      "Epoch 221/250 \t loss=0.0186 \t train_f1=0.9240 \t val_loss=0.0191 \t val_f1=0.9294 \t time=19.28s\n",
      "Epoch 231/250 \t loss=0.0182 \t train_f1=0.9256 \t val_loss=0.0210 \t val_f1=0.9264 \t time=20.82s\n",
      "Epoch 241/250 \t loss=0.0185 \t train_f1=0.9248 \t val_loss=0.0193 \t val_f1=0.9293 \t time=19.20s\n",
      "BEST VALIDATION SCORE (F1):  0.931120720805544\n",
      "starting fold 4\n",
      "###### splitting and reshaping the data\n",
      "(1000, 4000, 1)\n",
      "###### Loading\n",
      "Epoch 1/250 \t loss=0.3178 \t train_f1=0.0709 \t val_loss=0.2782 \t val_f1=0.0335 \t time=19.23s\n",
      "Epoch 11/250 \t loss=0.0348 \t train_f1=0.8209 \t val_loss=0.0406 \t val_f1=0.8255 \t time=20.01s\n",
      "Epoch 21/250 \t loss=0.0234 \t train_f1=0.9107 \t val_loss=0.0284 \t val_f1=0.9162 \t time=19.49s\n",
      "Epoch 31/250 \t loss=0.0211 \t train_f1=0.9197 \t val_loss=0.0261 \t val_f1=0.9211 \t time=19.01s\n",
      "Epoch 41/250 \t loss=0.0199 \t train_f1=0.9229 \t val_loss=0.0263 \t val_f1=0.9202 \t time=19.08s\n",
      "Epoch 51/250 \t loss=0.0203 \t train_f1=0.9178 \t val_loss=0.0267 \t val_f1=0.9082 \t time=19.28s\n",
      "Epoch 61/250 \t loss=0.0207 \t train_f1=0.9186 \t val_loss=0.0288 \t val_f1=0.9097 \t time=19.49s\n",
      "Epoch 71/250 \t loss=0.0197 \t train_f1=0.9215 \t val_loss=0.0262 \t val_f1=0.9127 \t time=19.13s\n",
      "Epoch 81/250 \t loss=0.0345 \t train_f1=0.8805 \t val_loss=0.0366 \t val_f1=0.9002 \t time=18.95s\n",
      "Epoch 91/250 \t loss=0.0204 \t train_f1=0.9232 \t val_loss=0.0261 \t val_f1=0.9208 \t time=19.10s\n",
      "Epoch 101/250 \t loss=0.0194 \t train_f1=0.9228 \t val_loss=0.0247 \t val_f1=0.9235 \t time=19.64s\n",
      "Epoch 111/250 \t loss=0.0196 \t train_f1=0.9207 \t val_loss=0.0249 \t val_f1=0.9218 \t time=19.81s\n",
      "Epoch 121/250 \t loss=0.0208 \t train_f1=0.9195 \t val_loss=0.0289 \t val_f1=0.9059 \t time=19.30s\n",
      "Epoch 131/250 \t loss=0.0190 \t train_f1=0.9241 \t val_loss=0.0241 \t val_f1=0.9235 \t time=19.30s\n",
      "Epoch 141/250 \t loss=0.0188 \t train_f1=0.9247 \t val_loss=0.0244 \t val_f1=0.9231 \t time=19.79s\n",
      "Epoch 151/250 \t loss=0.0197 \t train_f1=0.9216 \t val_loss=0.0257 \t val_f1=0.9203 \t time=19.78s\n",
      "Epoch 161/250 \t loss=0.0195 \t train_f1=0.9230 \t val_loss=0.0250 \t val_f1=0.9172 \t time=19.32s\n",
      "Epoch 171/250 \t loss=0.0184 \t train_f1=0.9257 \t val_loss=0.0245 \t val_f1=0.9236 \t time=19.58s\n",
      "Epoch 181/250 \t loss=0.0188 \t train_f1=0.9236 \t val_loss=0.0249 \t val_f1=0.9199 \t time=20.44s\n",
      "Epoch 191/250 \t loss=0.0190 \t train_f1=0.9204 \t val_loss=0.0246 \t val_f1=0.9204 \t time=19.39s\n",
      "Epoch 201/250 \t loss=0.0188 \t train_f1=0.9252 \t val_loss=0.0244 \t val_f1=0.9234 \t time=19.39s\n",
      "Epoch 211/250 \t loss=0.0182 \t train_f1=0.9258 \t val_loss=0.0238 \t val_f1=0.9238 \t time=20.55s\n",
      "Epoch 221/250 \t loss=0.0188 \t train_f1=0.9250 \t val_loss=0.0267 \t val_f1=0.9160 \t time=19.76s\n",
      "Epoch 231/250 \t loss=0.0182 \t train_f1=0.9255 \t val_loss=0.0241 \t val_f1=0.9217 \t time=19.65s\n",
      "Epoch 241/250 \t loss=0.0196 \t train_f1=0.9203 \t val_loss=0.0252 \t val_f1=0.9228 \t time=19.98s\n",
      "BEST VALIDATION SCORE (F1):  0.9258702590756225\n",
      "Final Score  0.9276811722568509\n"
     ]
    }
   ],
   "source": [
    "#Iterate through folds\n",
    "\n",
    "kf = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 42)\n",
    "\n",
    "local_val_score = 0\n",
    "models = {}\n",
    "\n",
    "k=0 #initialize fold number\n",
    "for tr_idx, val_idx in kfold.split(X, y):\n",
    "    test_p = np.zeros((int(test_input.shape[0] * test_input.shape[1])))\n",
    "\n",
    "    print('starting fold', k)\n",
    "    k += 1\n",
    "\n",
    "    print(6*'#', 'splitting and reshaping the data')\n",
    "    train_input = X[tr_idx]\n",
    "    print(train_input.shape)\n",
    "    train_target = y[tr_idx]\n",
    "    val_input = X[val_idx]\n",
    "    val_target = y[val_idx]\n",
    "    train_input_mean = train_input.mean()\n",
    "    train_input_sigma = train_input.std()\n",
    "    val_input = (val_input-train_input_mean)/train_input_sigma\n",
    "    train_input = (train_input-train_input_mean)/train_input_sigma\n",
    "\n",
    "    print(6*'#', 'Loading')\n",
    "    train = ION_Dataset_Sequential(train_input, train_target)\n",
    "    valid = ION_Dataset_Sequential(val_input, val_target)\n",
    "    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    #Build tensor data for torch\n",
    "    train_preds = np.zeros((int(train_input.shape[0] * train_input.shape[1])))\n",
    "    val_preds = np.zeros((int(val_input.shape[0] * val_input.shape[1])))\n",
    "    best_val_preds = np.zeros((int(val_input.shape[0] * val_input.shape[1])))\n",
    "    train_targets = np.zeros((int(train_input.shape[0] * train_input.shape[1])))\n",
    "    avg_losses_f = []\n",
    "    avg_val_losses_f = []\n",
    "\n",
    "    #Define loss function\n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    #Build model, initialize weights and define optimizer\n",
    "    model = Bi_RNN(lstm_input_size, hidden_state_size, batch_size=batch_size, output_dim=output_dim, num_layers=num_sequence_layers, rnn_type=rnn_type)  # (input_dim, hidden_state_size, batch_size, output_dim, num_seq_layers, rnn_type)\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)  # Using Adam optimizer\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min', patience=150, factor=0.1, min_lr=1e-8)  # Using ReduceLROnPlateau schedule\n",
    "    temp_val_loss = 9999999999\n",
    "    reached_val_score = 0\n",
    "\n",
    "    #Iterate through epochs\n",
    "    for epoch in range(n_epochs):\n",
    "        start_time = time.time()\n",
    "\n",
    "        #Train\n",
    "        model.train()\n",
    "        avg_loss = 0.\n",
    "        for i, (x_batch, y_batch) in enumerate(train_loader):\n",
    "            x_batch = x_batch.view(-1, num_time_steps, lstm_input_size)\n",
    "            y_batch = y_batch.view(-1, num_time_steps, output_dim)\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(x_batch.cuda())\n",
    "            loss = loss_fn(y_pred.cpu(), y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            avg_loss += loss.item() / len(train_loader)\n",
    "            pred = F.softmax(y_pred, 2).detach().cpu().numpy().argmax(axis=-1)\n",
    "            train_preds[i * batch_size * train_input.shape[1]:(i + 1) * batch_size * train_input.shape[1]] = pred.reshape((-1))\n",
    "            train_targets[i * batch_size * train_input.shape[1]:(i + 1) * batch_size * train_input.shape[1]] = y_batch.detach().cpu().numpy().argmax(axis=2).reshape((-1))\n",
    "            del y_pred, loss, x_batch, y_batch, pred\n",
    "\n",
    "        #Evaluate\n",
    "        model.eval()\n",
    "        avg_val_loss = 0.\n",
    "        for i, (x_batch, y_batch) in enumerate(valid_loader):\n",
    "            x_batch = x_batch.view(-1, num_time_steps, lstm_input_size)\n",
    "            y_batch = y_batch.view(-1, num_time_steps, output_dim)\n",
    "            y_pred = model(x_batch.cuda()).detach()\n",
    "            avg_val_loss += loss_fn(y_pred.cpu(), y_batch).item() / len(valid_loader)\n",
    "            pred = F.softmax(y_pred, 2).detach().cpu().numpy().argmax(axis=-1)\n",
    "            val_preds[i * batch_size * val_input.shape[1]:(i + 1) * batch_size * val_input.shape[1]] = pred.reshape((-1))\n",
    "            del y_pred, x_batch, y_batch, pred\n",
    "        if avg_val_loss < temp_val_loss:\n",
    "            temp_val_loss = avg_val_loss\n",
    "\n",
    "        #Calculate F1-score\n",
    "        train_score = f1_score(train_targets, train_preds, average='macro')\n",
    "        val_score = f1_score(val_target.argmax(axis=2).reshape((-1)), val_preds, average='macro')\n",
    "\n",
    "        #Print output of epoch\n",
    "        elapsed_time = time.time() - start_time\n",
    "        scheduler.step(avg_val_loss)\n",
    "        if epoch%10 == 0:\n",
    "            print('Epoch {}/{} \\t loss={:.4f} \\t train_f1={:.4f} \\t val_loss={:.4f} \\t val_f1={:.4f} \\t time={:.2f}s'.format(epoch + 1, n_epochs, avg_loss, train_score, avg_val_loss, val_score, elapsed_time))\n",
    "\n",
    "        if val_score > reached_val_score:\n",
    "            reached_val_score = val_score\n",
    "            best_model = copy.deepcopy(model.state_dict())\n",
    "            best_val_preds = copy.deepcopy(val_preds)\n",
    "\n",
    "    #Calculate F1-score of the fold\n",
    "    val_score_fold = f1_score(val_target.argmax(axis=2).reshape((-1)), best_val_preds, average='macro')\n",
    "\n",
    "    #Save the fold's model in a dictionary\n",
    "    models[k] = best_model\n",
    "\n",
    "    #Print F1-score of the fold\n",
    "    print(\"BEST VALIDATION SCORE (F1): \", val_score_fold)\n",
    "    local_val_score += (1/n_folds) * val_score_fold\n",
    "\n",
    "#Print final average k-fold CV F1-score\n",
    "print(\"Final Score \", local_val_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict test data by averaging model results from 5 folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Iterate through folds\n",
    "\n",
    "for k in range(n_folds):\n",
    "    test_p = np.zeros((int(test_input.shape[0] * test_input.shape[1])))\n",
    "    k += 1\n",
    "\n",
    "    #Import model of fold k\n",
    "    model = Bi_RNN(lstm_input_size, hidden_state_size, batch_size=batch_size, output_dim=output_dim, num_layers=num_sequence_layers, rnn_type=rnn_type)  # (input_dim, hidden_state_size, batch_size, output_dim, num_seq_layers, rnn_type)\n",
    "    model = model.to(device)\n",
    "    model.load_state_dict(models[k])\n",
    "\n",
    "    #Make predictions on test data\n",
    "    model.eval()\n",
    "    for i, x_batch in enumerate(test_loader):\n",
    "        x_batch = x_batch.view(-1, num_time_steps, lstm_input_size)\n",
    "        y_pred = model(x_batch.cuda()).detach()\n",
    "        pred = F.softmax(y_pred, 2).detach().cpu().numpy().argmax(axis=-1)\n",
    "        test_p[i * batch_size * test_input.shape[1]:(i + 1) * batch_size * test_input.shape[1]] = pred.reshape((-1))\n",
    "        del y_pred, x_batch, pred\n",
    "    test_preds += (1/n_folds) * test_p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create submission file\n",
    "df_sub = pd.read_csv(\"/kaggle/input/liverpool-ion-switching/sample_submission.csv\", dtype = {'time': str})\n",
    "df_sub.open_channels = np.array(test_preds, np.int)\n",
    "df_sub.to_csv(\"submission_bilstm.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
